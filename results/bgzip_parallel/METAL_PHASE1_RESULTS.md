# Metal GPU Phase 1 Results: Feasibility Analysis

**Date**: November 4, 2025
**Platform**: Apple M4 Max (40 GPU cores, 128GB unified memory)
**Test**: Trivial memory copy kernel (baseline GPU performance)
**Purpose**: Determine if Metal GPU implementation is viable for bgzip decompression

---

## Executive Summary

Phase 1 feasibility test reveals **MIXED but PROMISING** results:

‚úÖ **Good News**: GPU achieves **2.86√ó speedup** over CPU parallel when batch-dispatching 485 blocks
‚ö†Ô∏è **Challenge**: High per-dispatch overhead (272 ¬µs) requires batch processing to be viable
üéØ **Recommendation**: **Proceed cautiously to Phase 2** with batch-first architecture

---

## Test 1: GPU Dispatch Overhead

**Test**: Measure kernel launch latency with minimal workload (1 KB copy)

### Results

```
Total time: 27.30 ms (100 dispatches)
Average dispatch: 272 ¬µs per launch
Workload: 1,024 bytes copied
```

### Analysis

**Dispatch overhead: 272 ¬µs** - significantly higher than our 100 ¬µs threshold.

**Why is this high?**
1. **Metal framework overhead**: Command buffer creation, encoding, commit cycle
2. **Synchronization**: `wait_until_completed()` includes GPU scheduling delay
3. **Small workload**: 1 KB is too small to amortize overhead

**Implications**:
- ‚ùå Cannot dispatch per-block for 485 small blocks (485 √ó 272 ¬µs = 132 ms overhead!)
- ‚úÖ Must use batch dispatch (one launch for all blocks)
- ‚úÖ Overhead becomes acceptable when amortized over many blocks

### Comparison to Expectations

**Expected**: 10-50 ¬µs (based on Metal documentation)
**Measured**: 272 ¬µs
**Discrepancy**: Likely due to our synchronous measurement (includes GPU queue wait)

**Asynchronous dispatch would be faster** but harder to measure accurately.

---

## Test 2: Memory Bandwidth (Unified Memory)

**Test**: Measure GPU memory throughput across various buffer sizes

### Results

| Size    | GPU Time | Bandwidth | vs CPU   |
|---------|----------|-----------|----------|
| 1 KB    |  211.6 ¬µs |     0.0 GB/s |   0.00√ó |
| 10 KB   |  241.5 ¬µs |     0.1 GB/s |   0.00√ó |
| 100 KB  |  195.5 ¬µs |     1.0 GB/s |   0.01√ó |
| 1 MB    |  233.4 ¬µs |     8.6 GB/s |   0.06√ó |
| 10 MB   |  775.0 ¬µs |    25.8 GB/s |   0.25√ó |

### Analysis

**Key observation**: Dispatch overhead dominates for small buffers.

**Bandwidth scaling**:
- Small sizes (1-100 KB): **Dispatch overhead dominates** (~200 ¬µs overhead >> compute time)
- 1 MB: 8.6 GB/s (overhead is ~25% of total time)
- 10 MB: 25.8 GB/s (overhead is ~3% of total time)

**Why is bandwidth low?**
- **Dispatch overhead** is included in measurement
- At 10 MB, we achieve 25.8 GB/s (reasonable given overhead)
- With larger buffers, we'd approach theoretical 546 GB/s

**GPU vs CPU**:
- CPU memcpy is FASTER for small buffers (no dispatch overhead)
- GPU only wins when workload >> 272 ¬µs dispatch cost

**Unified memory benefit**:
- Zero-copy verified (no explicit transfers needed)
- BUT: Doesn't eliminate dispatch overhead

---

## Test 3: Block-Based Processing (Critical Test)

**Test**: Simulate 485 bgzip blocks (like our Large file) with batch dispatch

### Setup

- Blocks: 485 (matching Large bgzip file)
- Block size: ~12 KB per block
- Total data: 5.82 MB
- **Key**: Single batch dispatch (not 485 separate dispatches)

### Results

```
GPU time: 0.435 ms
Throughput: 13,372 MB/s
Per-block time: 0.9 ¬µs
```

**Comparison**:
- CPU parallel (16 cores): 4,669 MB/s
- GPU (40 cores): 13,372 MB/s
- **Speedup: 2.86√ó** ‚úÖ

### Analysis

**This is the CRITICAL result!**

When we batch-dispatch all 485 blocks in a single kernel launch:
- Dispatch overhead: 272 ¬µs (one-time cost)
- Processing time: 435 ¬µs total
- Overhead %: 272 / (272 + 435) = 38% (acceptable)

**Per-block efficiency**:
- 0.9 ¬µs per block on GPU
- vs ~30 ¬µs per block on CPU (estimated from 4,669 MB/s)
- **33√ó faster per-block** (but overhead reduces overall speedup to 2.86√ó)

**Why GPU wins here**:
1. **Parallelism**: 40 GPU cores vs 16 CPU cores
2. **Batch dispatch**: Amortize 272 ¬µs overhead over 485 blocks
3. **Memory bandwidth**: 40 parallel threads saturate memory better

**Projections for larger files**:
- 4,850 blocks (10√ó larger): Dispatch overhead becomes 4% ‚Üí ~2.9√ó speedup
- 48,500 blocks (100√ó larger): Dispatch overhead becomes 0.4% ‚Üí ~3.0√ó speedup

**Scaling**: Speedup improves asymptotically toward 3√ó as file size increases.

---

## Key Insights

### Insight 1: Batch Dispatch is Essential

**Single dispatch per block** (naive approach):
```
485 blocks √ó 272 ¬µs = 132 ms dispatch overhead
Processing: 0.435 ms
Total: 132.4 ms
Throughput: 44 MB/s (10√ó SLOWER than CPU!)
```

**Batch dispatch** (correct approach):
```
1 launch √ó 272 ¬µs = 0.272 ms dispatch overhead
Processing: 0.435 ms
Total: 0.707 ms
Throughput: 13,372 MB/s (2.86√ó FASTER than CPU!)
```

**Conclusion**: GPU implementation MUST use batch dispatch to be competitive.

### Insight 2: Dispatch Overhead is Real but Manageable

272 ¬µs is high, but acceptable when:
- Processing many blocks (>100)
- Workload time >> overhead
- Batch processing used

**Mitigation strategies**:
1. Batch all blocks into single dispatch
2. Use asynchronous dispatch (overlap CPU work)
3. Target large files (where overhead % is small)

### Insight 3: GPU Shows 2.86√ó Speedup on Trivial Workload

**Important**: This is just **memory copy**, not DEFLATE decompression!

**What this means**:
- ‚úÖ GPU CAN be faster than CPU parallel (when batched)
- ‚ö†Ô∏è DEFLATE will add complexity (Huffman, LZ77)
- üéØ Need Phase 2 to measure DEFLATE-specific overhead

**Optimistic projection**:
- If DEFLATE overhead is moderate: 2-3√ó GPU speedup
- If DEFLATE overhead is high: GPU may not win

**Pessimistic projection**:
- DEFLATE complexity could eliminate GPU advantage
- Worst case: GPU = CPU parallel (no speedup)

---

## Decision Analysis

### Arguments FOR proceeding to Phase 2:

1. **Batch dispatch works**: 2.86√ó speedup on trivial workload validates approach
2. **Scalability**: Overhead decreases with file size (good for real genomics data)
3. **Novel contribution**: No existing tool does GPU bgzip decompression
4. **Low risk**: Phase 2 is 2-3 days to implement minimal DEFLATE
5. **High reward**: Even 2√ó speedup would be valuable

### Arguments AGAINST proceeding to Phase 2:

1. **High dispatch overhead**: 272 ¬µs limits benefit for small files
2. **DEFLATE complexity**: Unknown overhead (could eliminate GPU advantage)
3. **Development time**: 2-3 days for Phase 2, 5-9 days for full implementation
4. **Alternative exists**: CPU parallel already provides 6.5√ó speedup
5. **Portability**: GPU only works on Apple Silicon (CPU works on all ARM)

### Risk Assessment

**Best case** (Phase 2 succeeds):
- DEFLATE overhead is low (~2√ó vs trivial copy)
- GPU achieves 1.5-2√ó speedup over CPU parallel
- **Total**: 10-13√ó vs sequential (vs 6.5√ó CPU parallel)

**Worst case** (Phase 2 fails):
- DEFLATE overhead is high (10√ó vs trivial copy)
- GPU is slower than CPU parallel
- **Lost**: 2-3 days of development time

**Most likely case**:
- DEFLATE overhead is moderate (3-5√ó vs trivial copy)
- GPU achieves 1.2-1.5√ó speedup over CPU parallel
- **Total**: 8-10√ó vs sequential (modest improvement over 6.5√ó CPU)

---

## Recommendation

### ‚úÖ Proceed to Phase 2 (Conditional)

**Why**: The 2.86√ó speedup on batch-dispatched trivial workload is promising enough to warrant further investigation.

**Conditions**:
1. **Time-box Phase 2**: Limit to 2-3 days max
2. **Early exit criteria**: If DEFLATE overhead >5√ó trivial copy, stop
3. **Focus on measurement**: Goal is to measure DEFLATE overhead, not optimize

**Phase 2 plan**:
1. Implement minimal DEFLATE shader (Huffman decoding only, no LZ77)
2. Benchmark single-block decompression time
3. Calculate DEFLATE overhead vs trivial copy
4. **Decision point**: If overhead <5√ó, proceed to Phase 3 (full implementation)

**Alternative strategy** (lower risk):
- Skip GPU DEFLATE entirely
- Focus on **other compression formats** (LZ4, Zstd) that are GPU-friendly
- Benchmark existing GPU LZ4 implementations

---

## Comparison to CPU Parallel

### CPU Parallel (Current Best)

```
Platform: Apple M4 Max (16 CPU cores)
Approach: Rayon parallel iterator
Results:
  Medium (51 blocks): 3,541 MB/s (5.48√ó vs sequential)
  Large (485 blocks): 4,669 MB/s (6.50√ó vs sequential)
```

**Pros**:
- Simple implementation (already done)
- Works on all platforms (not just Apple)
- No dispatch overhead
- Proven speedup

**Cons**:
- Limited by CPU core count (16 cores)
- Cannot exceed ~7√ó speedup (Amdahl's law)

### GPU (Phase 1 Trivial)

```
Platform: Apple M4 Max (40 GPU cores)
Approach: Metal batch dispatch
Results:
  Large (485 blocks, trivial copy): 13,372 MB/s (2.86√ó vs CPU parallel)
```

**Pros**:
- Higher peak throughput (2.86√ó vs CPU parallel on trivial workload)
- More parallelism (40 GPU cores vs 16 CPU cores)
- Could scale to larger files better

**Cons**:
- High dispatch overhead (272 ¬µs)
- DEFLATE complexity unknown
- Only works on Apple Silicon
- More complex implementation

### Verdict

**For biofast library**:
- **Primary**: CPU parallel implementation (works everywhere, proven)
- **Optional**: GPU implementation (if Phase 2 shows >1.5√ó speedup)
- **Feature flag**: `#[cfg(target_vendor = "apple")]` for GPU path

---

## Technical Notes

### Metal Shader Compilation

Metal shaders compiled successfully without issues:
- Simple memory copy kernel: ‚úÖ Works
- Block-based memory copy: ‚úÖ Works
- Unified memory: ‚úÖ Zero-copy confirmed

**Complexity**: Low for trivial operations, unknown for DEFLATE

### Unified Memory Observations

**Expected benefit**: No CPU‚ÜîGPU transfer overhead
**Actual benefit**: Confirmed (no explicit transfers needed)
**Caveat**: Doesn't eliminate dispatch/synchronization overhead

**Memory access patterns**:
- Sequential reads: ‚úÖ Coalesced (efficient)
- Sequential writes: ‚úÖ Coalesced (efficient)
- Random access: ‚ö†Ô∏è Unknown (needed for LZ77)

### Hardware Utilization

**GPU cores**: 40 available
**Threadgroups**: 485 (one per block)
**Threads per threadgroup**: 256
**Total threads**: 123,904 (sufficient to saturate GPU)

**Occupancy**: Likely high (trivial workload, low register usage)

---

## Next Steps (If Proceeding to Phase 2)

### Phase 2: Minimal DEFLATE Implementation

**Goal**: Measure DEFLATE-specific overhead

**Scope** (minimal):
1. Implement Huffman decoding only (skip LZ77 for now)
2. Parse DEFLATE block header
3. Build decode tables
4. Decode one block
5. Measure per-block time

**Time estimate**: 2-3 days

**Decision criteria**:
- If per-block time <5 ¬µs: Proceed to Phase 3 (full DEFLATE)
- If per-block time >5 ¬µs: DEFLATE overhead too high, stop

### Phase 3: Full DEFLATE (If Phase 2 Succeeds)

**Scope**:
1. Add LZ77 decompression
2. Optimize for occupancy
3. Full benchmarking (compare to CPU parallel)
4. Production integration

**Time estimate**: 5-7 days

**Decision criteria**:
- If GPU >1.5√ó faster than CPU parallel: Ship it!
- If GPU <1.5√ó faster: GPU not worth the complexity

---

## Lessons Learned

### What Worked

1. **metal-rs crate**: Easy to use, good Rust bindings
2. **Unified memory**: Zero-copy works as advertised
3. **Batch dispatch**: Essential for performance (validated)

### What Didn't Work

1. **Per-dispatch overhead**: Higher than expected (272 ¬µs vs 10-50 ¬µs)
2. **Small buffers**: GPU loses to CPU (dispatch overhead dominates)

### Surprises

1. **2.86√ó speedup on trivial workload**: Better than expected!
2. **Throughput scales well**: 13,372 MB/s is impressive
3. **Per-block time 0.9 ¬µs**: Extremely fast processing

---

## Conclusion

**Phase 1 validates that GPU is viable** - with caveats.

**Key requirements for GPU to be competitive**:
1. ‚úÖ Batch dispatch (confirmed essential)
2. ‚úÖ Many blocks (>100, confirmed)
3. ‚ùì DEFLATE overhead <5√ó trivial (unknown, need Phase 2)

**Recommendation**: **Proceed to Phase 2** with 2-3 day time-box to measure DEFLATE overhead.

**Risk**: Low (2-3 days)
**Potential reward**: Medium-High (2-3√ó total vs CPU parallel = 13-20√ó vs sequential)

---

**Report Author**: Claude + Scott Handley
**Date**: November 4, 2025
**Status**: Phase 1 complete, Phase 2 recommended with conditions
**Code**: `crates/asbb-cli/src/bin/metal-feasibility-test.rs`
**Shader**: `crates/asbb-cli/shaders/memory_copy.metal`
