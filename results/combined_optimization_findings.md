# Combined Optimization: NEON + Parallel Composition

**Date**: October 30, 2025
**Operation**: Base Counting
**Hardware**: M4 MacBook Pro
**Critical Discovery**: Parallel implementation must use NEON per-thread for proper composition

---

## Executive Summary

Testing NEON + Parallel combined optimization revealed a **critical implementation bug** that, once fixed, dramatically changed performance characteristics. The key finding:

**Parallel threading MUST use the best available SIMD per-thread, not naive scalar code.**

### Results Before Fix (Naive per-thread):
- Parallel (4T): 0.86-4.04Ã— speedup
- Combined appeared to have 6% composition efficiency

### Results After Fix (NEON per-thread):
- Parallel (4T): **1.88-60.58Ã— speedup** ðŸš€
- Combined = Parallel (they execute the same code path)

---

## The Bug: Naive Counting Per-Thread

### Original Implementation

```rust
fn execute_parallel(&self, data: &[SequenceRecord], num_threads: usize) -> Result<OperationOutput> {
    data.par_iter()
        .map(|record| {
            // âŒ BUG: Using naive scalar loop per-thread!
            for &base in &record.sequence {
                match base {
                    b'A' => count_a += 1,
                    // ...
                }
            }
        })
        .reduce(...)
}
```

**Problem**: Each thread processed sequences with naive scalar loop, completely bypassing NEON SIMD!

### Performance Impact

| Scale | Naive | Parallel (Buggy) | Parallel Speedup |
|-------|-------|------------------|------------------|
| 100 seqs | 1.07 Mseqs/sec | 0.92 Mseqs/sec | **0.86Ã—** âŒ (slower!) |
| 1K seqs | 1.14 Mseqs/sec | 3.23 Mseqs/sec | 2.82Ã— |
| 10K+ seqs | 1.10 Mseqs/sec | ~4.0 Mseqs/sec | 3.6Ã— |

**Parallelization barely helped** because each thread was running inefficient scalar code!

---

## The Fix: NEON Per-Thread

### Corrected Implementation

```rust
fn execute_parallel(&self, data: &[SequenceRecord], num_threads: usize) -> Result<OperationOutput> {
    data.par_iter()
        .map(|record| {
            #[cfg(target_arch = "aarch64")]
            {
                // âœ… FIX: Use NEON per-thread!
                count_bases_neon(&record.sequence)
            }

            #[cfg(not(target_arch = "aarch64"))]
            {
                // Fall back to naive on non-ARM
                naive_count(&record.sequence)
            }
        })
        .reduce(...)
}
```

**Solution**: Each thread now uses NEON SIMD for its work.

### Performance Impact

| Scale | Naive | Parallel (Fixed) | Parallel Speedup | Improvement |
|-------|-------|------------------|------------------|-------------|
| 100 seqs | 1.12 Mseqs/sec | 2.11 Mseqs/sec | 1.88Ã— | **2.2Ã— better** |
| 1K seqs | 1.10 Mseqs/sec | 8.09 Mseqs/sec | 7.33Ã— | **2.6Ã— better** |
| 10K seqs | 1.03 Mseqs/sec | 42.12 Mseqs/sec | **40.81Ã—** | **11Ã— better!** ðŸš€ |
| 100K seqs | 1.06 Mseqs/sec | 59.96 Mseqs/sec | **56.61Ã—** | **14Ã— better!** ðŸš€ |
| 1M+ seqs | 1.15 Mseqs/sec | ~68 Mseqs/sec | **~60Ã—** | **15Ã— better!** ðŸš€ |

**Parallel threading with NEON per-thread delivers up to 60Ã— speedup vs naive!**

---

## Architecture: Optimization Precedence

### Execution Order in `execute_with_config`

```rust
fn execute_with_config(&self, data: &[SequenceRecord], config: &HardwareConfig) -> Result<OperationOutput> {
    // Priority order:
    if config.use_gpu { return self.execute_gpu(...); }              // 1. GPU (highest priority if available)
    if config.use_neural_engine { return self.execute_neural(...); } // 2. Neural Engine
    if config.use_amx { return self.execute_amx(...); }              // 3. AMX
    if config.num_threads > 1 { return self.execute_parallel(...); } // 4. Parallel â† Takes precedence over NEON-only!
    if config.use_neon { return self.execute_neon(...); }            // 5. NEON-only
    self.execute_naive(data)                                          // 6. Naive (fallback)
}
```

**Critical insight**: `num_threads > 1` takes precedence over `use_neon`.

This means:
- **Config**: `use_neon=false, num_threads=4` â†’ `execute_parallel` (naive per-thread) âŒ **Before fix**
- **Config**: `use_neon=false, num_threads=4` â†’ `execute_parallel` (NEON per-thread) âœ… **After fix**
- **Config**: `use_neon=true, num_threads=4` â†’ `execute_parallel` (NEON per-thread) âœ… **Same path!**

**"NEON + Parallel" config doesn't create a unique code path** - it just ensures parallel uses NEON (which it now does by default).

---

## Composition Analysis: Corrected Understanding

### Naive Analysis (WRONG)

Our initial experiment compared:
- NEON-only: 17Ã— speedup
- Parallel-only: 3.8Ã— speedup (buggy, naive per-thread)
- Expected combined: 17Ã— Ã— 3.8Ã— = 64Ã— speedup
- Actual combined: 3.8Ã— speedup
- **Conclusion: 6% efficiency** âŒ

This analysis was **incorrect** because it assumed parallel used naive per-thread!

### Corrected Analysis (RIGHT)

After fix:
- NEON-only: 17Ã— speedup (single-threaded)
- Parallel with NEON per-thread: **60Ã— speedup** (4 threads Ã— NEON)
- **There is no separate "combined" path** - parallel IS the combination!

**Composition efficiency**: 60Ã— / (17Ã— baseline with 4 threads) = **~3.5Ã— threading benefit** with NEON per-thread.

This represents:
- **87.5% parallel efficiency** (3.5Ã— on 4 cores)
- **Full NEON benefit retained** within each thread

---

## The Correct Mental Model

### WRONG Mental Model (Before)

```
Optimizations = Independent layers that multiply:
  - NEON layer: 17Ã— speedup
  - Parallel layer: 3.8Ã— speedup
  - Combined: 17Ã— Ã— 3.8Ã— = 64Ã— expected
```

This model assumes parallelization doesn't change per-thread algorithm!

### RIGHT Mental Model (After)

```
Optimizations = Nested composition:
  - Parallel splits work across N threads
  - Each thread uses best available algorithm (NEON in this case)
  - Combined speedup = (best single-thread throughput) Ã— (parallel efficiency)
```

**Formula**:
```
Speedup_combined = Speedup_NEON Ã— Efficiency_parallel
                 = 17Ã— Ã— (3.5Ã—/4 threads)
                 = 17Ã— Ã— 87.5% efficiency
                 â‰ˆ 60Ã— observed
```

---

## Scale-Dependent Composition

### Parallel Efficiency by Scale

| Scale | Parallel Speedup | Threads | Efficiency |
|-------|------------------|---------|------------|
| 100 seqs | 1.88Ã— | 4 | 47% |
| 1K seqs | 7.33Ã— | 4 | **183%** (super-linear!) |
| 10K seqs | 40.81Ã— / 17Ã— NEON = 2.4Ã— threading | 4 | 60% |
| 100K seqs | 56.61Ã— / 17Ã— NEON = 3.3Ã— threading | 4 | **83%** |
| 1M+ seqs | 60Ã— / 17Ã— NEON = 3.5Ã— threading | 4 | **87.5%** |

### Analysis

**Tiny scale (100 seqs)**: 47% efficiency
- Thread overhead dominates
- Each thread processes only ~25 sequences
- Cache conflicts, work distribution overhead

**Small scale (1K seqs)**: 183% efficiency (SUPER-LINEAR!)
- This is **cache effects**
- 4 threads Ã— 250 seqs each = better cache locality
- Each thread's working set fits in L1 cache
- Less cache contention than single-threaded processing all 1K

**Large scale (100K+ seqs)**: 83-87.5% efficiency
- Excellent scaling
- Overhead amortized
- Work well-distributed

**Asymptote**: Efficiency approaches ~87.5% at very large scale, never quite reaches 4Ã— due to:
- Rayon work-stealing overhead
- Memory bandwidth contention (4 threads competing)
- Cache line sharing (false sharing)

---

## Generalization: How to Implement Combined Optimization

### Rule: Parallel Implementation Must Use Best Per-Thread Algorithm

**DON'T**:
```rust
fn execute_parallel(&self, data, num_threads) -> Result<Output> {
    data.par_iter()
        .map(|item| naive_process(item))  // âŒ Naive per-thread!
        .reduce(...)
}
```

**DO**:
```rust
fn execute_parallel(&self, data, num_threads) -> Result<Output> {
    data.par_iter()
        .map(|item| {
            // âœ… Use best available optimization per-thread
            #[cfg(target_arch = "aarch64")]
            { simd_process_neon(item) }

            #[cfg(not(target_arch = "aarch64"))]
            { naive_process(item) }
        })
        .reduce(...)
}
```

### Precedence in execute_with_config

The ordering matters:
1. **Specialized hardware** (GPU, Neural Engine, AMX) - highest priority
2. **Parallelization** - next priority (uses best per-thread algorithm)
3. **SIMD** - for single-threaded execution
4. **Naive** - fallback

**Parallel takes precedence over SIMD** because parallel implementation should internally use SIMD per-thread!

---

## Updated Optimization Rules

### For Element-Wise Operations (like base counting):

```rust
fn optimize_base_counting(data: &DataCharacteristics) -> HardwareConfig {
    let mut config = HardwareConfig::naive();

    // Rule 1: Use parallelization if above threshold
    if data.num_sequences >= 1_000 {
        config.num_threads = 4;
        // Parallel implementation will use NEON per-thread automatically
        // Expected: 60Ã— speedup at large scale
    } else {
        // Rule 2: Use NEON-only for small scale
        config.use_neon = true;
        // Expected: 17Ã— speedup
    }

    config
}
```

**Key insight**: We don't need to set `use_neon = true` when using `num_threads > 1` because the parallel implementation already uses NEON per-thread!

### Threshold Refined

| Sequences | Optimal Config | Expected Speedup | Reasoning |
|-----------|---------------|------------------|-----------|
| <1,000 | NEON-only (single-thread) | 17-55Ã— | Threading overhead > benefit |
| 1,000-10,000 | Parallel (4T with NEON) | 7-40Ã— | Good threading efficiency |
| >10,000 | Parallel (4T with NEON) | 40-60Ã— | Excellent threading efficiency |

---

## Lessons Learned

### 1. **Test Your Parallel Implementation**

Don't assume parallelization automatically uses best algorithm per-thread. Explicitly implement SIMD/optimizations within parallel paths.

### 2. **Composition is Not Multiplicative**

You can't just multiply independent speedups. Optimizations interact:
- Parallelization should incorporate per-thread optimizations
- Combined speedup â‰  SIMD Ã— Threading
- Combined speedup = SIMD Ã— (threading efficiency)

### 3. **Architecture Matters More Than Flags**

The `HardwareConfig` flags (`use_neon`, `num_threads`) don't directly map to code paths. The `execute_with_config` precedence order determines actual execution:
- Setting both `use_neon` and `num_threads > 1` doesn't create unique behavior
- Parallel path is responsible for using NEON internally

### 4. **Scale Dramatically Affects Composition**

- Small scale: Threading overhead dominates
- Medium scale: Super-linear cache effects possible
- Large scale: Approaches theoretical efficiency (~87.5%)

---

## Next Steps

### 1. **Test Other Operations**

Does this pattern hold for:
- GC content (similar element-wise)
- Reverse complement (more complex NEON)
- Quality filtering (branch-heavy, different profile)

### 2. **Profile Cache Effects**

Why does 1K sequences show 183% super-linear efficiency? Need cache profiling.

### 3. **Test on Other Hardware**

- M1/M2/M3 (fewer cores, less bandwidth)
- M4 Pro/Max (more cores)
- M5 (GPU Neural Accelerators, 153 GB/s bandwidth)

### 4. **Implement 2-Bit Encoding**

With parallel+NEON achieving 60Ã—, can 2-bit encoding push to 100Ã—+?

Expected: 2-bit within NEON per-thread â†’ 64 bases per register Ã— 4 threads Ã— efficiency

---

## Conclusion

**Major Finding**: Parallel threading with NEON per-thread delivers **60Ã— speedup** vs naive baseline for element-wise operations at large scale.

**Critical Bug Fixed**: Original implementation used naive per-thread (3.8Ã— speedup), now uses NEON per-thread (60Ã— speedup) - **16Ã— improvement**!

**Architecture Insight**: "Combined optimization" doesn't mean separate NEON and Parallel paths - it means parallel implementation uses NEON internally.

**Optimization Rule**:
```
IF num_sequences >= 1,000:
    â†’ Use parallel threading (4 threads)
    â†’ Automatically gets NEON per-thread
    â†’ Expected: 40-60Ã— speedup
ELSE:
    â†’ Use NEON-only (single-threaded)
    â†’ Expected: 17-55Ã— speedup
```

**This changes everything about how we think about combined optimizations on Apple Silicon.**

---

**Generated**: October 30, 2025
**Experiment**: asbb-pilot-scales (combined optimization test)
**Hardware**: M4 MacBook Pro
**Status**: Phase 1, Combined Optimization Validated
