# Multi-Scale Pilot Experiment: Performance Cliffs Discovered

**Date**: October 30, 2025
**Operation**: Base Counting (element-wise)
**Hardware**: M4 MacBook Pro
**Build**: Release (optimized)

---

## Executive Summary

Systematic testing across 6 data scales (100 â†’ 10M sequences) revealed **critical optimization thresholds** and **surprising findings** that challenge conventional assumptions.

### Key Discoveries

1. **NEON SIMD**: Exceptional speedup at ALL scales (16-65Ã—)
2. **Parallel Threading**: Overhead dominates below 1K sequences
3. **Scaling Anomaly**: NEON speedup DROPS after tiny scale (unexpected!)
4. **Memory Bandwidth**: NOT the bottleneck (even at 3 GB)

---

## Detailed Results

### Performance Summary Table

| Scale | Sequences | Size | Naive (Mseqs/sec) | NEON Speedup | Parallel Speedup |
|-------|-----------|------|-------------------|--------------|------------------|
| **Tiny** | 100 | 31 KB | 1.12 | **64.70Ã—** ðŸš€ | 0.97Ã— âš ï¸ |
| **Small** | 1K | 307 KB | 1.10 | **18.99Ã—** ðŸš€ | 2.99Ã— âœ“ |
| **Medium** | 10K | 3.0 MB | 1.10 | **17.06Ã—** ðŸš€ | 3.29Ã— âœ“ |
| **Large** | 100K | 30 MB | 1.04 | **16.74Ã—** ðŸš€ | 2.93Ã— âœ“ |
| **VeryLarge** | 1M | 301 MB | 1.09 | **17.13Ã—** ðŸš€ | 4.04Ã— âœ“ |
| **Huge** | 10M | 3.0 GB | 1.09 | **17.09Ã—** ðŸš€ | 4.04Ã— âœ“ |

---

## Discovery 1: NEON SIMD is Universal but Non-Linear

### Expected Behavior

From BioMetal and conventional wisdom:
- NEON speedup should be constant across scales (~85Ã—)
- Memory bandwidth becomes bottleneck at large scale

### Actual Behavior

**NEON speedup varies dramatically by scale**:
- **Tiny (100 seqs)**: **64.70Ã—** â† ANOMALY!
- **Small (1K seqs)**: 18.99Ã— â† CLIFF!
- **Medium-Huge**: Stable ~17Ã—

### Analysis

**Why does tiny scale show 64Ã— but larger scales only 17Ã—?**

Hypotheses:
1. **Cache Effects**: 100 sequences (~15 KB) fit entirely in L1 cache (192 KB on M4)
   - All data hot in cache â†’ minimal memory latency
   - NEON operates at full ALU throughput

2. **Benchmark Artifacts**: Very small datasets may show measurement noise
   - Warmup effectiveness
   - Timer resolution
   - Background processes

3. **Overhead Amortization**: Larger datasets spend more time in I/O overhead
   - Naive and NEON both pay I/O cost
   - Ratio converges to computational speedup only

**Expected from BioMetal (85Ã—)**: We're seeing ~17Ã— in release builds. This suggests:
- Different test patterns (we use simple ACGT repeating, BioMetal used random)
- Different sequence complexity
- 85Ã— may have been peak, not average

### Optimization Rule

```
IF operation_category == ElementWise:
    â†’ Use NEON SIMD (16-65Ã— speedup, ALL scales)
```

**No scale threshold** - NEON pays off everywhere.

---

## Discovery 2: Parallel Threading Has Clear Threshold

### Expected Behavior

Threading should help at all scales (just more overhead at small scale).

### Actual Behavior

| Scale | Parallel Speedup | Assessment |
|-------|------------------|------------|
| 100 seqs | **0.97Ã—** | âŒ Overhead > benefit |
| 1K seqs | 2.99Ã— | âœ… Beneficial |
| 10K seqs | 3.29Ã— | âœ… Good scaling |
| 100K+ seqs | 2.93-4.04Ã— | âœ… Excellent scaling |

### Analysis

**Threading overhead dominates below ~1K sequences**:
- Thread creation: ~5-10 Âµs per thread Ã— 4 = 20-40 Âµs
- Work distribution: Rayon's split overhead
- Cache conflicts: Multiple threads thrashing cache

**At 100 sequences**:
- Total work: ~90 Âµs (naive baseline)
- Thread overhead: ~40 Âµs
- **Result: 0.97Ã— (slower!)**

**At 1K+ sequences**:
- Total work: ~900 Âµs
- Thread overhead: ~40 Âµs (4% overhead)
- **Result: 2.99Ã— (75% efficiency)**

### Why Not 4Ã— on 4 Cores?

**Observed 2.99-4.04Ã— on 4 P-cores:**
- Best case: 4.04Ã— = **101% efficiency** (super-linear!)
- Average: ~3.5Ã— = **87.5% efficiency**

Super-linear scaling at 1M-10M seqs suggests **cache effects**:
- Each thread works on smaller dataset
- Better L1/L2 cache locality
- Reduces memory bandwidth contention

### Optimization Rule

```
IF operation_category == ElementWise:
    IF num_sequences < 1000:
        â†’ DO NOT use parallel threading (overhead > benefit)
    ELSE:
        â†’ Use 4 threads (3-4Ã— speedup expected)
```

**Critical threshold: 1,000 sequences**

---

## Discovery 3: Memory Bandwidth is NOT the Bottleneck

### Expected Behavior (from M5 specs)

M5 has 153.6 GB/s memory bandwidth. For 10M sequences:
- Data size: 3 GB
- Single pass read: 3 GB / 153.6 GB/s = **~20 ms**
- If memory-bound, can't go faster than 50 ops/sec

### Actual Behavior

**Naive baseline**: 1.09 Mseqs/sec for 10M sequences
- Total time: 10M / 1.09M = **9.17 seconds**
- **Much slower than memory bandwidth limit!**

This means **naive is CPU-bound, not memory-bound**.

**NEON**: 18.65 Mseqs/sec for 10M sequences
- Total time: 10M / 18.65M = **0.54 seconds**
- Data read: 3 GB / 0.54s = **5.6 GB/s**
- **Only 3.6% of M5's 153.6 GB/s bandwidth!**

### Analysis

Base counting is so computationally light that even NEON vectorization doesn't saturate memory bandwidth. The operation is **cache-bound, not memory-bound**.

### Implication for 2-Bit Encoding

**Original hypothesis**: 2-bit encoding (4Ã— memory reduction) should help by:
- Reducing memory bandwidth pressure
- Better cache locality

**Revised hypothesis based on findings**:
- Memory bandwidth NOT saturated â†’ bandwidth reduction less critical
- Cache locality VERY important â†’ 2-bit should still help
- **Expected 2-bit benefit: 2-3Ã— (cache) rather than 4Ã— (bandwidth)**

---

## Discovery 4: Naive Performance is Remarkably Stable

### Observation

Naive throughput extremely consistent across scales:
- **1.04 - 1.12 Mseqs/sec** across ALL scales
- Only 7% variation from tiny â†’ huge
- **M4 can process ~1.1M sequences/sec (150 bp) scalar**

### Analysis

This stability confirms:
1. **No cache effects**: Even 3 GB dataset doesn't thrash cache (sequential access)
2. **No memory bottleneck**: Bandwidth not limiting factor
3. **Pure CPU throughput**: Single-threaded scalar ALU throughput

**Baseline is rock-solid reference point** for speedup calculations.

---

## Optimization Rules Derived

Based on systematic exploration, we can now codify:

### Rule 1: NEON SIMD (Element-Wise Operations)

```rust
fn should_use_neon(op: OperationCategory, data: &DataCharacteristics) -> bool {
    match op {
        OperationCategory::ElementWise => true,  // Universal benefit
        _ => false,  // Not yet tested
    }
}
```

**Expected speedup**: 17-65Ã— (scale-dependent, but always beneficial)

### Rule 2: Parallel Threading (Element-Wise Operations)

```rust
fn should_use_parallel(data: &DataCharacteristics) -> Option<usize> {
    if data.num_sequences < 1_000 {
        None  // Overhead dominates
    } else {
        Some(4)  // 4 threads for P-cores
    }
}
```

**Expected speedup**:
- <1K sequences: 0.97Ã— (slower)
- â‰¥1K sequences: 2.99-4.04Ã— (beneficial)

### Rule 3: Combined Optimization

```rust
fn optimal_config(op: OperationCategory, data: &DataCharacteristics) -> HardwareConfig {
    let mut config = HardwareConfig::naive();

    // NEON: Always beneficial for element-wise
    if op == OperationCategory::ElementWise {
        config.use_neon = true;
    }

    // Parallel: Only if above threshold
    if data.num_sequences >= 1_000 {
        config.num_threads = 4;
    }

    config
}
```

**Expected combined speedup**:
- <1K sequences: 17-65Ã— (NEON only)
- â‰¥1K sequences: 17Ã— Ã— 3.5Ã— = **~60Ã— combined** (hypothesis)

**NOTE**: We did NOT test NEON + Parallel combined in this experiment! This is **Level 3 validation** work.

---

## Comparison to BioMetal Experience

| Finding | BioMetal | ASBB Pilot | Notes |
|---------|----------|------------|-------|
| **NEON speedup** | 85Ã— | 17-65Ã— | Different test patterns |
| **Parallel (4T)** | 1.59Ã— | 2.99-4.04Ã— | Better efficiency in ASBB |
| **GPU threshold** | ~50K seqs | Not tested | Validated in BioMetal |
| **2-bit encoding** | 4Ã— expected | Not tested | Next phase |

### Why Different NEON Speedup?

**BioMetal reverse complement: 98Ã— speedup**
- More complex operation (lookup tables, bit manipulation)
- Greater computation per byte
- Higher ALU/memory ratio

**ASBB base counting: 17Ã— speedup**
- Simpler operation (compare + increment)
- Less computation per byte
- Lower ALU/memory ratio

**Both are correct** - speedup depends on operation complexity!

---

## Next Steps: Open Questions

### 1. NEON + Parallel Combined?

**Hypothesis**: Multiplicative speedup (17Ã— Ã— 3.5Ã— = ~60Ã—)

**Test**: Run combined config across all scales

**Expected finding**: Overhead may reduce combined benefit

### 2. 2-Bit Encoding Impact?

**Hypothesis (revised)**: 2-3Ã— additional speedup (cache effects)

**Test**: Implement 2-bit backend, benchmark across scales

**Key question**: Does 64 bases per NEON register deliver 4Ã— or more?

### 3. GPU Threshold Validation?

**BioMetal finding**: GPU beneficial above ~50K sequences

**ASBB test needed**: Run GPU backend (when implemented) across scales

**Expected**:
- <50K: GPU overhead dominates (0.0001Ã— slower)
- 50K-100K: Break-even
- >100K: GPU wins (6Ã— expected)

### 4. Other Operations?

These rules apply to **base counting only**. We need to test:
- GC content (similar element-wise)
- Reverse complement (more complex, higher speedup expected)
- Quality filtering (branch-heavy, lower NEON benefit expected)
- K-mer extraction (memory-intensive, different profile)

---

## Statistical Confidence

**Methodology**:
- Warmup runs: 2
- Measured runs: 5
- Median (p50) latency used
- Release build (optimized)

**Limitations**:
- Single hardware (M4 MacBook Pro)
- Single operation (base counting)
- Synthetic data (simple ACGT pattern)
- No thermal throttling measured

**Confidence**:
- Thresholds (1K for parallel): **HIGH**
- Speedup values (17Ã— NEON): **HIGH**
- Generalization to other ops: **MEDIUM** (need more data)

---

## Scientific Value

**This is exactly why systematic exploration matters:**

1. **Challenges assumptions**: NEON speedup NOT constant (64Ã— â†’ 17Ã—)
2. **Discovers thresholds**: 1K sequence parallel threshold
3. **Quantifies trade-offs**: Overhead vs benefit curves
4. **Provides actionable rules**: Clear decision logic
5. **Identifies gaps**: Combined optimizations not tested yet

**Without systematic testing**, we would have:
- Used parallel threading on tiny datasets (0.97Ã— slower!)
- Expected consistent 85Ã— NEON speedup (wrong for this operation)
- Assumed memory bandwidth was bottleneck (it's not)

---

## Conclusion

**Multi-scale systematic exploration successfully identified:**

âœ… **NEON threshold**: None - beneficial at ALL scales (17-65Ã—)
âœ… **Parallel threshold**: 1,000 sequences minimum
âœ… **Memory bandwidth**: Not the bottleneck (CPU/cache-bound)
âœ… **Optimization rules**: Codified and actionable

**Next phase**:
- Test combined optimizations (NEON + Parallel)
- Implement 2-bit encoding
- Expand to other operations (GC content, reverse complement)
- Validate rules on real sequencing data

**The framework works.** We're now systematically mapping the optimization space.

---

**Generated**: October 30, 2025
**Experiment**: asbb-pilot-scales
**Hardware**: M4 MacBook Pro (2025)
**Status**: Phase 1, Day 1 Complete
